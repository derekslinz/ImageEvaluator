--- image_eval_embed.py
+++ image_eval_embed.py
@@ -4,51 +4,51 @@ import argparse
 import base64
 import csv
 import hashlib
 import io
 import json  # Import json for parsing the response
 import logging
 import logging.handlers
 import os
 import pickle
 import re
 import subprocess
 import sys
 import time
 import warnings
 from contextlib import contextmanager, nullcontext
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 # Suppress FutureWarning from timm (used by PyIQA)
 warnings.filterwarnings("ignore", message="Importing from timm.models.layers is deprecated")
 
 import math
 import numpy as np
 import requests
-from PIL import Image, ImageStat
+from PIL import Image, ImageFilter, ImageStat
 from colorama import Fore, Style
 from pydantic import BaseModel, ConfigDict, field_validator
 from tqdm import tqdm
 from profile_config import (
     PROFILE_CONFIG,
     get_profile,
     get_profile_name,
     PYIQA_BASELINE_STATS,
     PROFILE_SCORE_CENTER,
     PROFILE_SCORE_STD_SCALE,
     get_default_pyiqa_shift,
 )
 
 try:
     import piexif  # type: ignore
     import piexif.helper  # type: ignore
     PIEXIF_AVAILABLE = True
 except ImportError:  # pragma: no cover - optional dependency
     piexif = None  # type: ignore
     PIEXIF_AVAILABLE = False
 
 try:
     import cv2  # type: ignore
     CV2_AVAILABLE = True
 except ImportError:  # pragma: no cover - optional dependency
@@ -89,91 +89,102 @@ def list_pyiqa_metrics() -> List[str]:
 
 # Increase PIL decompression bomb limit for large legitimate images
 Image.MAX_IMAGE_PIXELS = None  # Remove limit entirely (or set to a higher value like 500000000)
 
 
 # =============================================================================
 # Constants
 # =============================================================================
 
 # PyIQA configuration
 PYIQA_MAX_LONG_EDGE = 1024
 DEFAULT_CLIPIQ_MODEL = "clipiqa+_vitL14_512"
 
 # Stock evaluation thresholds
 STOCK_MIN_RESOLUTION_MP = 4.0
 STOCK_RECOMMENDED_MP = 12.0
 STOCK_MIN_DPI = 300
 STOCK_DPI_FIXABLE = 240
 
 # Technical metric baseline statistics (mean, std) for outlier detection
 # Thresholds are calculated as: mean ± 1.5*std
 # These can be recalibrated based on your image corpus
 TECHNICAL_BASELINES = {
     # Sharpness: lower is worse, flag if < mean - 1.5*std
     "sharpness": {"mean": 45.0, "std": 20.0},  # threshold ~15
-    # Noise: higher is worse, flag if > mean + 1.5*std  
+    # Noise: higher is worse, flag if > mean + 1.5*std
     "noise_score": {"mean": 30.0, "std": 15.0},  # threshold ~52.5
     # Highlight clipping %: higher is worse, flag if > mean + 1.5*std
     "histogram_clipping_highlights": {"mean": 2.0, "std": 5.0},  # threshold ~9.5
     # Shadow clipping %: higher is worse, flag if > mean + 1.5*std
     "histogram_clipping_shadows": {"mean": 2.0, "std": 5.0},  # threshold ~9.5
     # Color cast delta: higher is worse, flag if > mean + 1.5*std
-    "color_cast_delta": {"mean": 8.0, "std": 6.0},  # threshold ~17
+    # Calibrated to a broader corpus (mean≈42, std≈32) so warn/critical trigger on strong casts.
+    "color_cast_delta": {"mean": 42.0, "std": 32.0},  # warn ~74, critical ~90
 }
 
 # Multiplier for outlier detection (1.5 = mild outliers, 2.0 = strong outliers)
 OUTLIER_SIGMA_MULTIPLIER = 1.5
 
 
 def get_technical_threshold(metric_name: str, direction: str = "high") -> float:
     """Calculate threshold for a metric based on baseline statistics.
     
     Args:
         metric_name: Name of the metric in TECHNICAL_BASELINES
         direction: "high" for metrics where higher is worse (noise, clipping)
                    "low" for metrics where lower is worse (sharpness)
     
     Returns:
         Threshold value (mean ± 1.5*std)
     """
     baseline = TECHNICAL_BASELINES.get(metric_name, {"mean": 50.0, "std": 25.0})
     mean = baseline["mean"]
     std = baseline["std"]
     
     if direction == "low":
         return mean - (OUTLIER_SIGMA_MULTIPLIER * std)
     else:  # high
         return mean + (OUTLIER_SIGMA_MULTIPLIER * std)
 
 
-# Legacy thresholds (kept for backward compatibility, now computed dynamically)
-STOCK_SHARPNESS_CRITICAL = get_technical_threshold("sharpness", "low")
+# Derived thresholds for reporting and rules
+SHARPNESS_CRITICAL_THRESHOLD = get_technical_threshold("sharpness", "low")
+SHARPNESS_WARN_THRESHOLD = TECHNICAL_BASELINES["sharpness"]["mean"] - TECHNICAL_BASELINES["sharpness"]["std"]
+NOISE_WARN_THRESHOLD = TECHNICAL_BASELINES["noise_score"]["mean"] + TECHNICAL_BASELINES["noise_score"]["std"]
+NOISE_HIGH_THRESHOLD = get_technical_threshold("noise_score", "high")
+CLIP_HIGHLIGHT_THRESHOLD = get_technical_threshold("histogram_clipping_highlights", "high")
+CLIP_SHADOW_THRESHOLD = get_technical_threshold("histogram_clipping_shadows", "high")
+COLOR_CAST_WARN_THRESHOLD = TECHNICAL_BASELINES["color_cast_delta"]["mean"] + TECHNICAL_BASELINES["color_cast_delta"]["std"]
+COLOR_CAST_CRITICAL_THRESHOLD = get_technical_threshold("color_cast_delta", "high")
+
+# Legacy thresholds (kept for backward compatibility)
+STOCK_SHARPNESS_CRITICAL = SHARPNESS_CRITICAL_THRESHOLD
 STOCK_SHARPNESS_OPTIMAL = TECHNICAL_BASELINES["sharpness"]["mean"]
 STOCK_NOISE_WARN = TECHNICAL_BASELINES["noise_score"]["mean"]
-STOCK_NOISE_HIGH = get_technical_threshold("noise_score", "high")
-STOCK_CLIPPING_THRESHOLD = get_technical_threshold("histogram_clipping_highlights", "high")
+STOCK_NOISE_HIGH = NOISE_HIGH_THRESHOLD
+STOCK_CLIPPING_THRESHOLD = CLIP_HIGHLIGHT_THRESHOLD
 
 # Score validation bounds
 SCORE_MIN = 1
 SCORE_MAX = 100
 
 # Histogram analysis thresholds
 HISTOGRAM_HIGHLIGHT_START = 250
 HISTOGRAM_SHADOW_END = 6
 
 STOCK_EVALUATION_PROMPT = """You are a senior stock photography reviewer for major agencies (Adobe Stock, Shutterstock, Getty).
 Given the attached image and technical summary, output STRICT JSON with these keys:
 {"COMMERCIAL_VIABILITY": int, "TECHNICAL_QUALITY": int, "COMPOSITION_CLARITY": int, "KEYWORD_POTENTIAL": int,
  "RELEASE_CONCERNS": int, "REJECTION_RISKS": int, "OVERALL_STOCK_SCORE": int,
  "RECOMMENDATION": "EXCELLENT|GOOD|MARGINAL|REJECT", "PRIMARY_CATEGORY": "Business|Lifestyle|Nature|Food|Technology|Travel|Other",
  "NOTES": "short sentence", "ISSUES": "comma list"}
 
 Guidelines:
 - Scores 0-100 integers. Be conservative; mediocre images cluster 50-65.
 - COMMERCIAL_VIABILITY: demand, versatility, timelessness.
 - TECHNICAL_QUALITY: exposure, sharpness, noise, DR.
 - COMPOSITION_CLARITY: subject readability, copy space, framing.
 - KEYWORD_POTENTIAL: count of accurate, high-demand concepts.
 - RELEASE_CONCERNS: legal risk (people/property/logos). 100 = safe.
 - REJECTION_RISKS: likelihood image passes review. 100 = very likely to pass (noise, focus, clichés).
 - OVERALL_STOCK_SCORE = 0.4*TQ + 0.25*CV + 0.2*CC + 0.1*KP + 0.05*(RC+RR)/2, then subtract 5 points for highlight or shadow clipping >12% and 5 points for sharpness<30 (stackable). Clamp 0-100.
@@ -382,54 +393,56 @@ def apply_profile_rules(profile_key: str, technical_metrics: Dict[str, Any]) ->
     if isinstance(sharpness, (int, float)) and sharpness_rules:
         if sharpness < sharpness_rules.get("critical_threshold", -float("inf")):
             penalty += sharpness_rules.get("critical_penalty", 0.0)
             adjustments.append("sharpness_critical")
         elif sharpness < sharpness_rules.get("soft_threshold", -float("inf")):
             penalty += sharpness_rules.get("soft_penalty", 0.0)
             adjustments.append("sharpness_soft")
 
     clipping = technical_metrics.get("histogram_clipping_highlights")
     clipping_rules = rules.get("clipping")
     if isinstance(clipping, (int, float)) and clipping_rules:
         if clipping > clipping_rules.get("hard_pct", float("inf")):
             penalty += clipping_rules.get("hard_penalty", 0.0)
             adjustments.append("clipping_hard")
         elif clipping > clipping_rules.get("warn_pct", float("inf")):
             penalty += clipping_rules.get("warn_penalty", 0.0)
             adjustments.append("clipping_warn")
         elif clipping_rules.get("bonus_pct") is not None and clipping < clipping_rules.get("bonus_pct", 0.0):
             bonus = clipping_rules.get("bonus_points", 0.0)
             if bonus:
                 penalty -= bonus
                 adjustments.append("clipping_bonus")
 
     color_cast_label = technical_metrics.get("color_cast")
     color_cast_delta = technical_metrics.get("color_cast_delta", 0.0)
-    color_rules = rules.get("color_cast")
-    if color_rules and color_cast_label and color_cast_label != "neutral":
-        if color_cast_delta >= color_rules.get("threshold", 0.0):
-            penalty += color_rules.get("penalty", 0.0)
+    color_rules = rules.get("color_cast") or {}
+    color_threshold = color_rules.get("threshold", COLOR_CAST_WARN_THRESHOLD)
+    color_penalty = color_rules.get("penalty", 0.0)
+    if color_cast_label and color_cast_label != "neutral":
+        if color_cast_delta >= color_threshold:
+            penalty += color_penalty
             adjustments.append("color_cast")
 
     brightness = technical_metrics.get("brightness")
     brightness_rules = rules.get("brightness")
     if isinstance(brightness, (int, float)) and brightness_rules:
         min_ok = brightness_rules.get("min_ok")
         max_ok = brightness_rules.get("max_ok")
         mild = brightness_rules.get("mild_penalty", 0.0)
         strong = brightness_rules.get("strong_penalty", mild)
         tolerance = 20.0
         if min_ok is not None and brightness < min_ok:
             delta = min_ok - brightness
             apply = strong if delta > tolerance else mild
             if apply:
                 penalty += apply
                 adjustments.append("brightness_low_strong" if delta > tolerance else "brightness_low")
         elif max_ok is not None and brightness > max_ok:
             delta = brightness - max_ok
             apply = strong if delta > tolerance else mild
             if apply:
                 penalty += apply
                 adjustments.append("brightness_high_strong" if delta > tolerance else "brightness_high")
 
     return penalty, adjustments
 
@@ -1275,51 +1288,51 @@ def _warn_rawpy_missing():
             "rawpy is not installed; RAW files (e.g., NEF, CR2, ARW) will be skipped unless rawpy is available."
         )
         RAWPY_IMPORT_WARNINGED = True
 
 def _get_int_env(var_name: str, fallback: int) -> int:
     """Read an integer environment variable, falling back if unset/invalid."""
     value = os.environ.get(var_name)
     if value is None:
         return fallback
     try:
         return int(value)
     except ValueError:
         logger.warning(f"Ignoring invalid value for {var_name}: {value!r}. Using {fallback}.")
         return fallback
 
 # Configuration constants
 DEFAULT_IMAGE_FOLDER = os.environ.get("IMAGE_EVAL_DEFAULT_FOLDER", str(Path.cwd()))
 DEFAULT_OLLAMA_URL = os.environ.get("IMAGE_EVAL_OLLAMA_URL", "http://localhost:11434/api/generate")
 DEFAULT_WORKER_COUNT = _get_int_env("IMAGE_EVAL_WORKERS", 4)
 MAX_RETRIES = 3
 RETRY_DELAY_BASE = 2  # seconds
 DEFAULT_MODEL = "qwen3-vl:8b"
 CACHE_DIR = ".image_eval_cache"
 CACHE_VERSION = "v1"
 # Technical analysis constants - now using statistical thresholds from TECHNICAL_BASELINES
-COLOR_CAST_THRESHOLD = get_technical_threshold("color_cast_delta", "high")
+COLOR_CAST_THRESHOLD = COLOR_CAST_WARN_THRESHOLD
 HISTOGRAM_HIGHLIGHT_RANGE = (250, 256)  # Histogram bins considered as highlights
 HISTOGRAM_SHADOW_RANGE = (0, 6)  # Histogram bins considered as shadows
 MIN_LONG_EDGE = 850  # Minimum pixels required on the long edge
 
 
 class ImageResolutionTooSmallError(RuntimeError):
     """Raised when an image fails the minimum long-edge requirement."""
 
 
 def _extract_pil_exif_metadata(image_path: str) -> Dict[str, Union[str, int, None]]:
     """Extract EXIF metadata using PIL - works for both RAW (NEF/CR2) and JPEG/PNG.
     
     Note: Lens information may not be available for all RAW files as it's often stored in
     manufacturer-specific MakerNote tags that PIL doesn't parse. Use exiftool for complete
     lens metadata extraction if needed.
     """
     metadata: Dict[str, Union[str, int, None]] = {}
     
     try:
         with Image.open(image_path) as img:
             exif = img.getexif()
             if not exif:
                 return metadata
             
             # For RAW files, EXIF data is in a separate IFD (tag 0x8769)
@@ -2029,143 +2042,169 @@ def analyze_image_technical(image_path: str, iso_value: Optional[int] = None, co
             metrics['histogram_clipping_highlights'] = max(highlights_per_channel)
             metrics['histogram_clipping_shadows'] = max(shadows_per_channel)
 
             # --- Color cast detection (global) ---
             r_mean, g_mean, b_mean = stat.mean[:3]
             max_diff = max(abs(r_mean - g_mean), abs(g_mean - b_mean), abs(r_mean - b_mean))
             metrics['color_cast_delta'] = float(max_diff)
             # We only set the label here; the profile decides how/if to penalize.
             if max_diff > COLOR_CAST_THRESHOLD:
                 # Identify dominant channel with sufficient margin (>5 units)
                 if r_mean > g_mean + 5 and r_mean > b_mean + 5:
                     metrics['color_cast'] = 'warm/red'
                 elif b_mean > r_mean + 5 and b_mean > g_mean + 5:
                     metrics['color_cast'] = 'cool/blue'
                 elif g_mean > r_mean + 5 and g_mean > b_mean + 5:
                     metrics['color_cast'] = 'green'
                 else:
                     # max_diff > threshold but no clear dominant channel
                     metrics['color_cast'] = 'mixed'
 
             # --- Sharpness via Laplacian variance (scale-normalized) ---
             img_gray = img_rgb.convert('L')
             gray_array = np.array(img_gray)
 
             if CV2_AVAILABLE and cv2 is not None:
-                laplacian_var = cv2.Laplacian(gray_array, cv2.CV_64F).var()
-
-                # Scale-normalize: adjust for image resolution
-                # Larger images naturally have higher variance, normalize to 1MP baseline
-                h, w = gray_array.shape
-                mp = (h * w) / 1_000_000
-                scale_factor = np.sqrt(max(mp, 0.1))  # Avoid division by zero
-                raw_sharpness = laplacian_var / scale_factor
-                
-                # Map to 0-100 scale using square root transform
-                # sqrt compresses outliers less aggressively than log
-                # Calibrated: sqrt(25)=5->25, sqrt(100)=10->50, sqrt(400)=20->100
-                import math
-                sqrt_sharp = math.sqrt(max(0, raw_sharpness))
-                # Scale so sqrt(400)=20 maps to 100
-                normalized = sqrt_sharp * 5.0
-                metrics['sharpness'] = float(max(0, min(100, normalized)))
-
-                # --- Camera/ISO-agnostic noise estimation ---
-                # 1) Standardize size
-                target_long = 2048
-                scale = target_long / float(max(h, w))
-                if scale < 1.0:
-                    gray_small = cv2.resize(
-                        gray_array,
-                        dsize=None,
-                        fx=scale,
-                        fy=scale,
-                        interpolation=cv2.INTER_AREA,
+                try:
+                    laplacian_var = cv2.Laplacian(gray_array, cv2.CV_64F).var()
+
+                    # Scale-normalize: adjust for image resolution
+                    # Larger images naturally have higher variance, normalize to 1MP baseline
+                    h, w = gray_array.shape
+                    mp = (h * w) / 1_000_000
+                    scale_factor = np.sqrt(max(mp, 0.1))  # Avoid division by zero
+                    raw_sharpness = laplacian_var / scale_factor
+
+                    # Map to 0-100 scale using square root transform
+                    # sqrt compresses outliers less aggressively than log
+                    # Calibrated: sqrt(25)=5->25, sqrt(100)=10->50, sqrt(400)=20->100
+                    sqrt_sharp = math.sqrt(max(0, raw_sharpness))
+                    # Scale so sqrt(400)=20 maps to 100
+                    normalized = sqrt_sharp * 5.0
+                    metrics['sharpness'] = float(max(0, min(100, normalized)))
+
+                    # --- Camera/ISO-agnostic noise estimation ---
+                    # 1) Standardize size
+                    target_long = 2048
+                    scale = target_long / float(max(h, w))
+                    if scale < 1.0:
+                        gray_small = cv2.resize(
+                            gray_array,
+                            dsize=None,
+                            fx=scale,
+                            fy=scale,
+                            interpolation=cv2.INTER_AREA,
+                        )
+                    else:
+                        gray_small = gray_array
+
+                    # 2) Normalize to [0,1]
+                    gray_f = gray_small.astype(np.float32)
+                    max_val = gray_f.max()
+                    if max_val > 1.5:  # assume 8-bit-equivalent
+                        gray_f /= 255.0
+                    elif max_val > 0:  # already 0–1
+                        gray_f /= max_val  # safety normalization on odd inputs
+
+                    # 3) High-frequency residual
+                    blurred = cv2.GaussianBlur(gray_f, (0, 0), sigmaX=1.0, sigmaY=1.0)
+                    residual = gray_f - blurred
+
+                    # 4) Mask to flat regions (avoid edges and extremes)
+                    gx = cv2.Sobel(gray_f, cv2.CV_32F, 1, 0, ksize=3)
+                    gy = cv2.Sobel(gray_f, cv2.CV_32F, 0, 1, ksize=3)
+                    grad_mag = np.sqrt(gx**2 + gy**2)
+
+                    edge_thresh = np.percentile(grad_mag, 75.0)
+                    flat_mask = grad_mag < edge_thresh
+
+                    p_low, p_high = np.percentile(gray_f, [5.0, 95.0])
+                    luminance_mask = (gray_f > p_low) & (gray_f < p_high)
+
+                    final_mask = flat_mask & luminance_mask
+                    flat_residuals = residual[final_mask]
+
+                    # Fallback: need at least 1% of pixels for reliable noise estimate
+                    min_pixels = int(0.01 * residual.size)
+                    if flat_residuals.size < min_pixels:
+                        # If too few flat pixels, use full image but log it
+                        flat_residuals = residual.flatten()
+                        logger.debug(
+                            "Noise estimation using full image for %s (insufficient flat regions)",
+                            image_path,
+                        )
+
+                    # Robust sigma via MAD
+                    med = float(np.median(flat_residuals))  # type: ignore
+                    mad = float(np.median(np.abs(flat_residuals - med)))  # type: ignore
+                    if mad < 1e-6:
+                        sigma_noise = 0.0
+                    else:
+                        sigma_noise = 1.4826 * mad  # approx std for Gaussian
+
+                    # 5) Normalize by effective dynamic range
+                    p1, p99 = np.percentile(gray_f, [1.0, 99.0])
+                    dynamic_range = max(float(p99 - p1), 1e-6)
+                    relative_noise = sigma_noise / dynamic_range
+
+                    # 6) Map to 0–100 severity score
+                    REL_NOISE_MIN = 0.002
+                    REL_NOISE_MAX = 0.04
+                    rn_clipped = min(max(relative_noise, REL_NOISE_MIN), REL_NOISE_MAX)
+                    noise_score = (rn_clipped - REL_NOISE_MIN) / (REL_NOISE_MAX - REL_NOISE_MIN) * 100.0
+
+                    metrics['noise_sigma'] = float(sigma_noise)
+                    metrics['noise_score'] = float(noise_score)
+                    metrics['noise'] = float(noise_score)  # backward-compatible alias
+                except Exception as e:
+                    logger.debug(
+                        "OpenCV sharpness/noise failed for %s (%s); using numpy/PIL fallback",
+                        image_path,
+                        e,
                     )
-                else:
-                    gray_small = gray_array
-
-                # 2) Normalize to [0,1]
-                gray_f = gray_small.astype(np.float32)
-                max_val = gray_f.max()
-                if max_val > 1.5:  # assume 8-bit-equivalent
-                    gray_f /= 255.0
-                elif max_val > 0:  # already 0–1
-                    gray_f /= max_val  # safety normalization on odd inputs
-
-                # 3) High-frequency residual
-                blurred = cv2.GaussianBlur(gray_f, (0, 0), sigmaX=1.0, sigmaY=1.0)
-                residual = gray_f - blurred
-
-                # 4) Mask to flat regions (avoid edges and extremes)
-                gx = cv2.Sobel(gray_f, cv2.CV_32F, 1, 0, ksize=3)
-                gy = cv2.Sobel(gray_f, cv2.CV_32F, 0, 1, ksize=3)
-                grad_mag = np.sqrt(gx**2 + gy**2)
-
-                edge_thresh = np.percentile(grad_mag, 75.0)
-                flat_mask = grad_mag < edge_thresh
-
-                p_low, p_high = np.percentile(gray_f, [5.0, 95.0])
-                luminance_mask = (gray_f > p_low) & (gray_f < p_high)
-
-                final_mask = flat_mask & luminance_mask
-                flat_residuals = residual[final_mask]
-
-                # Fallback: need at least 1% of pixels for reliable noise estimate
-                min_pixels = int(0.01 * residual.size)
-                if flat_residuals.size < min_pixels:
-                    # If too few flat pixels, use full image but log it
-                    flat_residuals = residual.flatten()
-                    logger.debug(f"Noise estimation using full image for {image_path} (insufficient flat regions)")
-
-                # Robust sigma via MAD
-                med = float(np.median(flat_residuals))  # type: ignore
-                mad = float(np.median(np.abs(flat_residuals - med)))  # type: ignore
-                if mad < 1e-6:
-                    sigma_noise = 0.0
-                else:
-                    sigma_noise = 1.4826 * mad  # approx std for Gaussian
-
-                # 5) Normalize by effective dynamic range
-                p1, p99 = np.percentile(gray_f, [1.0, 99.0])
-                dynamic_range = max(float(p99 - p1), 1e-6)
-                relative_noise = sigma_noise / dynamic_range
-
-                # 6) Map to 0–100 severity score
-                REL_NOISE_MIN = 0.002
-                REL_NOISE_MAX = 0.04
-                rn_clipped = min(max(relative_noise, REL_NOISE_MIN), REL_NOISE_MAX)
-                noise_score = (rn_clipped - REL_NOISE_MIN) / (REL_NOISE_MAX - REL_NOISE_MIN) * 100.0
-
-                metrics['noise_sigma'] = float(sigma_noise)
-                metrics['noise_score'] = float(noise_score)
-                metrics['noise'] = float(noise_score)  # backward-compatible alias
+                    sharpness, noise_score, sigma_noise = _compute_sharpness_noise_numpy(
+                        gray_array, image_path
+                    )
+                    metrics['sharpness'] = sharpness
+                    metrics['noise_score'] = noise_score
+                    metrics['noise_sigma'] = sigma_noise
+                    metrics['noise'] = noise_score
             else:
                 logger.debug("OpenCV not available, skipping Laplacian/denoise metrics for %s", image_path)
-                # Fallback sharpness proxy using grayscale standard deviation
-                gray_stat = ImageStat.Stat(img_gray)
-                metrics['sharpness'] = float(sum(gray_stat.stddev) / len(gray_stat.stddev))
+                sharpness, noise_score, sigma_noise = _compute_sharpness_noise_numpy(gray_array, image_path)
+                metrics['sharpness'] = sharpness
+                metrics['noise_score'] = noise_score
+                metrics['noise_sigma'] = sigma_noise
+                metrics['noise'] = noise_score
+
+            # If metrics are still zero (e.g., blank images), attempt a conservative fallback once more
+            if metrics['sharpness'] <= 0 or metrics['noise_score'] <= 0:
+                s_val, n_score, n_sigma = _compute_sharpness_noise_numpy(gray_array, image_path)
+                metrics['sharpness'] = max(metrics['sharpness'], s_val)
+                metrics['noise_score'] = max(metrics['noise_score'], n_score)
+                metrics['noise_sigma'] = max(metrics['noise_sigma'], n_sigma)
+                metrics['noise'] = metrics['noise_score']
     
     except ImageResolutionTooSmallError:
         raise
     except Exception as e:
         logger.debug(f"Could not analyze technical metrics for {image_path}: {e}")
         metrics['status'] = 'error'
     
     return metrics
 
 
 def assess_technical_metrics(technical_metrics: Dict, context: str = "studio_photography") -> List[str]:
     """Generate human-readable warnings based on measured metrics and context."""
     profile = get_profile(context)
     rules = profile.get("rules", {})
     warnings: List[str] = []
 
     # Sharpness
     sharpness = technical_metrics.get('sharpness')
     sharpness_rules = rules.get("sharpness", {})
     if sharpness is not None:
         if sharpness < sharpness_rules.get("critical_threshold", 30.0):
             warnings.append(f"Sharpness critically low ({sharpness:.1f})")
         elif sharpness < sharpness_rules.get("soft_threshold", 60.0):
             warnings.append(f"Lower sharpness ({sharpness:.1f}) may impact detail")
 
@@ -2234,50 +2273,143 @@ def compute_post_process_potential(technical_metrics: Dict, context: str = "stud
     elif clipping < clip_bonus:
         base_score += 5
     # else: neutral zone (between clip_bonus and clip_mid) - no adjustment
 
     # Noise contribution
     noise_score = float(technical_metrics.get('noise_score', 0.0))
     noise_rules = rules.get("noise", {})
     noise_penalty_high = post_process.get("noise_penalty_high", 15)
     noise_penalty_mid = post_process.get("noise_penalty_mid", 5)
     
     if noise_score > noise_rules.get("high", 60.0):
         base_score -= noise_penalty_high
     elif noise_score > noise_rules.get("warn", 30.0):
         base_score -= noise_penalty_mid
 
     # Color cast contribution
     color_cast = technical_metrics.get('color_cast', 'neutral')
     if color_cast != 'neutral':
         color_penalty = rules.get("color_cast", {}).get("penalty", 5)
         base_score -= color_penalty
 
     post_score = max(0, min(100, int(round(base_score))))
     return post_score
 
 
+# =============================================================================
+# Technical metric helpers
+# =============================================================================
+
+
+def _compute_sharpness_noise_numpy(gray_array: np.ndarray, image_path: str) -> Tuple[float, float, float]:
+    """Compute sharpness and noise estimates without relying on OpenCV.
+
+    Args:
+        gray_array: Grayscale image as a NumPy array.
+        image_path: Path for logging context.
+
+    Returns:
+        sharpness (0–100), noise_score (0–100), noise_sigma (absolute residual sigma).
+    """
+    h, w = gray_array.shape
+    gray_float = gray_array.astype(np.float32)
+
+    # Laplacian variance using padded discrete kernel (avoids wrap-around artifacts)
+    padded = np.pad(gray_float, 1, mode='edge')
+    laplacian = (
+        -4 * gray_float
+        + padded[1:-1, :-2]
+        + padded[1:-1, 2:]
+        + padded[:-2, 1:-1]
+        + padded[2:, 1:-1]
+    )
+    laplacian_var = float(laplacian.var())
+
+    mp = (h * w) / 1_000_000
+    scale_factor = math.sqrt(max(mp, 0.1))
+    raw_sharpness = laplacian_var / scale_factor
+    sqrt_sharp = math.sqrt(max(0, raw_sharpness))
+    normalized_sharpness = sqrt_sharp * 5.0
+    sharpness_score = float(max(0, min(100, normalized_sharpness)))
+
+    # Basic noise estimate using Gaussian blur residuals
+    target_long = 2048
+    scale = target_long / float(max(h, w))
+    if scale < 1.0:
+        gray_small = np.array(
+            Image.fromarray(gray_array).resize(
+                (int(w * scale), int(h * scale)), resample=Image.BILINEAR
+            )
+        )
+    else:
+        gray_small = gray_array
+
+    gray_f = gray_small.astype(np.float32) / 255.0
+    blurred = np.array(
+        Image.fromarray(gray_small).filter(ImageFilter.GaussianBlur(radius=1.0)),
+        dtype=np.float32,
+    ) / 255.0
+    residual = gray_f - blurred
+
+    gx = np.zeros_like(gray_f)
+    gy = np.zeros_like(gray_f)
+    gx[:, 1:] = np.diff(gray_f, axis=1)
+    gy[1:, :] = np.diff(gray_f, axis=0)
+    grad_mag = np.sqrt(gx**2 + gy**2)
+
+    edge_thresh = np.percentile(grad_mag, 75.0)
+    flat_mask = grad_mag < edge_thresh
+
+    p_low, p_high = np.percentile(gray_f, [5.0, 95.0])
+    luminance_mask = (gray_f > p_low) & (gray_f < p_high)
+
+    final_mask = flat_mask & luminance_mask
+    flat_residuals = residual[final_mask]
+    min_pixels = int(0.01 * residual.size)
+    if flat_residuals.size < min_pixels:
+        flat_residuals = residual.flatten()
+        logger.debug("Noise estimation using full image for %s (insufficient flat regions)", image_path)
+
+    med = float(np.median(flat_residuals))  # type: ignore
+    mad = float(np.median(np.abs(flat_residuals - med)))  # type: ignore
+    if mad < 1e-6:
+        sigma_noise = 0.0
+    else:
+        sigma_noise = 1.4826 * mad
+
+    p1, p99 = np.percentile(gray_f, [1.0, 99.0])
+    dynamic_range = max(float(p99 - p1), 1e-6)
+    relative_noise = sigma_noise / dynamic_range
+
+    REL_NOISE_MIN = 0.002
+    REL_NOISE_MAX = 0.04
+    rn_clipped = min(max(relative_noise, REL_NOISE_MIN), REL_NOISE_MAX)
+    noise_score = (rn_clipped - REL_NOISE_MIN) / (REL_NOISE_MAX - REL_NOISE_MIN) * 100.0
+
+    return sharpness_score, float(noise_score), float(sigma_noise)
+
+
 def analyze_image_with_context(image_path: str, ollama_host_url: str, model: str,
                                context_override: Optional[str], skip_context_classification: bool,
                                stock_eval: bool = False
                                ) -> Tuple[str, Dict, Dict, List[str]]:
     """Determine context, extract EXIF, and compute technical metrics for an image."""
     if context_override:
         image_context = context_override if context_override in PROFILE_CONFIG else 'studio_photography'
         logger.info(f"Using manual context override: {image_context}")
     else:
         cached_context = read_cached_context(image_path)
         if cached_context:
             image_context = cached_context
             logger.info(f"Using cached context from EXIF: {image_context}")
         elif skip_context_classification:
             image_context = 'studio_photography'
             logger.debug(f"Context classification disabled, using default: {image_context}")
         else:
             try:
                 image_context = classify_image_context(image_path, ollama_host_url, model)
             except Exception as e:
                 logger.warning(f"Context classification failed for {image_path}: {e}, using default")
                 image_context = 'studio_photography'
 
     logger.info(f"Image context for {image_path}: {image_context} ({PROFILE_CONFIG[image_context]['name']})")
 
@@ -3342,116 +3474,141 @@ def load_results_from_csv(csv_path: str) -> List[Tuple[str, Optional[Dict]]]:
                     'technical_warnings': row.get('technical_warnings', '').split('; ') if row.get('technical_warnings') else [],
                     'post_process_potential': row.get('post_process_potential', ''),
                 }
             results.append((row.get('file_path', ''), metadata))
     return results
 
 
 def get_cache_stats(cache_dir: str) -> Dict:
     """Get cache statistics."""
     if not cache_dir or not os.path.exists(cache_dir):
         return {'enabled': False, 'entries': 0, 'size_mb': 0}
     
     try:
         cache_files = list(Path(cache_dir).glob('*.cache'))
         total_size = sum(f.stat().st_size for f in cache_files)
         return {
             'enabled': True,
             'entries': len(cache_files),
             'size_mb': total_size / (1024 * 1024)
         }
     except Exception as e:
         logger.warning(f"Error getting cache stats: {e}")
         return {'enabled': False, 'entries': 0, 'size_mb': 0}
 
 
+TECH_METRIC_THRESHOLDS: Dict[str, List[Dict[str, Any]]] = {
+    'sharpness': [
+        {'label': 'CRITICAL', 'threshold': SHARPNESS_CRITICAL_THRESHOLD, 'comparison': '<'},
+        {'label': 'warn', 'threshold': SHARPNESS_WARN_THRESHOLD, 'comparison': '<'},
+    ],
+    'noise_score': [
+        {'label': 'warn', 'threshold': NOISE_WARN_THRESHOLD, 'comparison': '>'},
+        {'label': 'HIGH', 'threshold': NOISE_HIGH_THRESHOLD, 'comparison': '>'},
+    ],
+    'histogram_clipping_highlights': [
+        {'label': 'CLIPPING', 'threshold': CLIP_HIGHLIGHT_THRESHOLD, 'comparison': '>'},
+    ],
+    'histogram_clipping_shadows': [
+        {'label': 'CLIPPING', 'threshold': CLIP_SHADOW_THRESHOLD, 'comparison': '>'},
+    ],
+    'color_cast_delta': [
+        {'label': 'warn', 'threshold': COLOR_CAST_WARN_THRESHOLD, 'comparison': '>'},
+        {'label': 'CRITICAL', 'threshold': COLOR_CAST_CRITICAL_THRESHOLD, 'comparison': '>'},
+    ],
+}
+
+
 def compute_metric_stats(values: List[float]) -> Dict[str, Any]:
     """Compute comprehensive statistics for a list of values."""
     if not values:
         return {}
     n = len(values)
     sorted_vals = sorted(values)
     mean = sum(values) / n
     variance = sum((v - mean) ** 2 for v in values) / n if n > 1 else 0
     std_dev = variance ** 0.5
-    
+
     # Quartiles
     median = sorted_vals[n // 2] if n % 2 == 1 else (sorted_vals[n // 2 - 1] + sorted_vals[n // 2]) / 2
     q1 = sorted_vals[n // 4] if n >= 4 else sorted_vals[0]
     q3 = sorted_vals[(3 * n) // 4] if n >= 4 else sorted_vals[-1]
-    
+
     return {
         'count': n,
         'mean': mean,
         'std': std_dev,
         'min': min(values),
         'max': max(values),
         'median': median,
         'q1': q1,
         'q3': q3,
         'values': values,  # Keep raw values for histogram
     }
 
 
-def build_histogram(values: List[float], bins: int = 10, max_width: int = 40) -> List[Tuple[str, int, str]]:
+def build_histogram(values: List[float], bins: int = 10, max_width: int = 40) -> List[Tuple[str, int, str, float, float]]:
     """Build ASCII histogram bins from values.
     
     Returns list of (bin_label, count, bar_string) tuples.
     """
     if not values:
         return []
     
     min_val = min(values)
     max_val = max(values)
     
     # Handle edge case where all values are the same
     if min_val == max_val:
         return [(f"{min_val:.1f}", len(values), '█' * max_width)]
     
     bin_width = (max_val - min_val) / bins
     histogram: List[Tuple[str, int]] = []
     
     for i in range(bins):
         bin_start = min_val + i * bin_width
         bin_end = bin_start + bin_width
         if i == bins - 1:
             # Last bin includes max value
             count = sum(1 for v in values if bin_start <= v <= bin_end)
         else:
             count = sum(1 for v in values if bin_start <= v < bin_end)
         label = f"{bin_start:.1f}-{bin_end:.1f}"
         histogram.append((label, count))
     
     # Build bar strings
     max_count = max(c for _, c in histogram) if histogram else 1
     scale = max_count / max_width if max_count > max_width else 1
     
     result = []
-    for label, count in histogram:
+    for i, (label, count) in enumerate(histogram):
+        # Recover bin edges for marker placement
+        bin_start = min_val + i * bin_width
+        bin_end = bin_start + bin_width
         bar_len = max(1, int(count / scale)) if count > 0 else 0
         bar = '█' * bar_len
-        result.append((label, count, bar))
+        result.append((label, count, bar, bin_start, bin_end))
     
     return result
 
 
 def calculate_statistics(results: List[Tuple[str, Optional[Dict]]]) -> Dict:
     """Calculate statistics from processing results."""
     scores = []
     tech_scores: List[int] = []
     raw_count = 0
     pil_count = 0
     potentials = []
     
     # Collect all technical metrics
     technical_metrics_collected: Dict[str, List[float]] = {
         'sharpness': [],
         'noise_score': [],
         'histogram_clipping_highlights': [],
         'histogram_clipping_shadows': [],
         'color_cast_delta': [],
         'brightness': [],
         'contrast': [],
         'megapixels': [],
     }
     
     # Collect all IQA metric scores
@@ -3583,68 +3740,130 @@ def calculate_statistics(results: List[Tuple[str, Optional[Dict]]]) -> Dict:
         'median_score': median,
         'std_dev': std_dev,
         'q1': q1,
         'q3': q3,
         'min_score': min(scores) if scores else 0,
         'max_score': max(scores) if scores else 0,
         'score_distribution': distribution,
         'raw_count': raw_count,
         'pil_count': pil_count,
         'warning_images': warning_images,
         'avg_post_process_potential': sum(potentials)/len(potentials) if potentials else 0,
         'technical_score_distribution': tech_distribution,
         'avg_technical_score': tech_avg,
         'technical_min_score': tech_min,
         'technical_max_score': tech_max,
         'technical_median_score': tech_median,
         'technical_q1': tech_q1,
         'technical_q3': tech_q3,
         'technical_std_dev': tech_std,
         # New: comprehensive metric statistics
         'technical_metrics_stats': {k: compute_metric_stats(v) for k, v in technical_metrics_collected.items() if v},
         'iqa_metrics_stats': {k: compute_metric_stats(v) for k, v in iqa_metrics_collected.items() if v},
     }
 
 
+def _count_threshold_flags(values: List[float], thresholds: List[Dict[str, Any]]) -> List[Tuple[str, int, float]]:
+    """Return list of (label, count, pct) for threshold violations."""
+    flagged: List[Tuple[str, int, float]] = []
+    total = len(values)
+    if total == 0:
+        return flagged
+
+    for rule in thresholds:
+        label = rule.get('label', '')
+        threshold = float(rule.get('threshold', 0))
+        comparison = rule.get('comparison', '>')
+
+        if comparison == '<':
+            count = sum(1 for v in values if v < threshold)
+        else:
+            count = sum(1 for v in values if v > threshold)
+
+        pct = (count / total) * 100 if total else 0
+        flagged.append((label, count, pct))
+
+    return flagged
+
+
+def _find_bin_marker(bin_start: float, bin_end: float, thresholds: List[Dict[str, Any]]) -> Optional[str]:
+    """Return the most specific threshold label that falls within a histogram bin."""
+    markers: List[Tuple[float, str, str]] = []
+    for rule in thresholds:
+        label = rule.get('label', '')
+        threshold = float(rule.get('threshold', 0))
+        comparison = rule.get('comparison', '>')
+        if bin_start <= threshold <= bin_end:
+            markers.append((threshold, label, comparison))
+
+    if not markers:
+        return None
+
+    # Prefer the highest threshold for '>' rules, lowest for '<'
+    greater_markers = [m for m in markers if m[2] == '>']
+    less_markers = [m for m in markers if m[2] == '<']
+
+    if greater_markers:
+        return sorted(greater_markers, key=lambda x: x[0], reverse=True)[0][1]
+    if less_markers:
+        return sorted(less_markers, key=lambda x: x[0])[0][1]
+
+    # Fallback to first marker
+    return markers[0][1]
+
+
 def print_metric_stats(name: str, stat: Dict[str, Any], show_histogram: bool = True):
     """Print statistics and histogram for a single metric."""
     if not stat:
         return
-    
-    print(f"\n  {name}:")
+
+    thresholds = TECH_METRIC_THRESHOLDS.get(name, [])
+    threshold_desc = ""
+    if thresholds:
+        parts = [f"{rule['label']}{rule['comparison']}{rule['threshold']:.1f}" for rule in thresholds]
+        threshold_desc = f"  [Thresholds: {', '.join(parts)}]"
+
+    print(f"\n  {name}:{threshold_desc}")
     print(f"    Count: {stat['count']}")
     print(f"    Mean: {stat['mean']:.2f}  Std: {stat['std']:.2f}")
     print(f"    Range: {stat['min']:.2f} - {stat['max']:.2f}")
     print(f"    Quartiles: Q1={stat['q1']:.2f}  Median={stat['median']:.2f}  Q3={stat['q3']:.2f}")
-    
+
+    if thresholds and stat.get('values'):
+        flagged = _count_threshold_flags(stat['values'], thresholds)
+        if flagged:
+            flagged_parts = [f"{label}: {count} ({pct:.1f}%)" for label, count, pct in flagged]
+            print(f"    Flagged: {', '.join(flagged_parts)}")
+
     if show_histogram and stat.get('values'):
         histogram = build_histogram(stat['values'], bins=10, max_width=30)
         if histogram:
             print(f"    Distribution:")
-            for label, count, bar in histogram:
-                if count > 0:
-                    print(f"      {label:>15}: {bar} ({count})")
+            for label, count, bar, bin_start, bin_end in histogram:
+                marker = _find_bin_marker(bin_start, bin_end, thresholds)
+                marker_text = f" ◄{marker}" if marker else ""
+                print(f"      {label:>15}: {bar} ({count}){marker_text}")
 
 
 def print_statistics(stats: Dict):
     """Print formatted summary statistics to console with color coding."""
     """Print formatted statistics."""
     print(f"\n{'='*60}")
     print(f"PROCESSING SUMMARY")
     print(f"{'='*60}")
     print(f"Total images processed: {stats['total_processed']}")
     print(f"Successful: {stats['successful']}")
     print(f"Failed: {stats['failed']}")
     if stats.get('warning_images'):
         print(f"Images with technical warnings: {stats['warning_images']}")
     print(f"RAW formats (exiftool): {stats.get('raw_count', 0)}")
     print(f"Standard formats (PIL): {stats.get('pil_count', 0)}")
     
     # Add timing information
     if 'elapsed_time' in stats:
         elapsed = stats['elapsed_time']
         print(f"\n{'='*60}")
         print(f"TIMING STATISTICS")
         print(f"{'='*60}")
         print(f"Total processing time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)")
         if stats['total_processed'] > 0:
             print(f"Time per image: {elapsed/stats['total_processed']:.2f} seconds")
